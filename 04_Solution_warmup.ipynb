{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247666fb",
   "metadata": {},
   "source": [
    "NB 03 didn't quite work.\n",
    "\n",
    "The two likely reasons for that are:\n",
    " - issues with how I am generating train data\n",
    " - false assumption that you can toss whatever at a ranking model and it will do the rest\n",
    "\n",
    "In this notebook, I want to put the groundwork needed for growing a good solution. First of all, that will require a robust and fast local validation scheme. We know that using the last week of train data for validation work and tracks the leaderboard nicely.\n",
    "\n",
    "Secondly, we want to start from a kernel of a solution that we can extend. This [notebook](https://www.kaggle.com/hengzheng/time-is-our-best-friend-v2) on kaggle seems to me like a great starting point.\n",
    "\n",
    "The plan is to develop the functionality needed for a nice setup of a solution that we will reuse in NB 05. Along the way I hope to learn a bit more about the data, about some of the trends that I might want to model through the features I will engineer.\n",
    "\n",
    "The plan is:\n",
    "* implement a quick training pipeline leading to good validation\n",
    "* train a ranking model on candidates we know to be good\n",
    "* only generate new training data / candidates while validating whether we are moving in the right direction using local CV\n",
    "* start with building sensible features and see whether they move the needle on the score\n",
    "\n",
    "The truth is I do not know what will work. These RecSys models are a completely new breed of models to me. But I can set the problem up in a way as to help me learn. And that is what I am going to do :).\n",
    "\n",
    "Once I get this working I will breathe a sigh of relief and will jump into reading papers and drawing inspiration from there.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3ac989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from average_precision import apk\n",
    "\n",
    "# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/308635\n",
    "def customer_hex_id_to_int(series):\n",
    "    return series.str[-16:].apply(hex_id_to_int)\n",
    "\n",
    "def hex_id_to_int(str):\n",
    "    return int(str[-16:], 16)\n",
    "\n",
    "def article_id_str_to_int(series):\n",
    "    return series.astype('int32')\n",
    "\n",
    "def article_id_int_to_str(series):\n",
    "    return '0' + series.astype('str')\n",
    "\n",
    "class Categorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_examples=0):\n",
    "        self.min_examples = min_examples\n",
    "        self.categories = []\n",
    "        \n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            vc = X.iloc[:, i].value_counts()\n",
    "            self.categories.append(vc[vc > self.min_examples].index.tolist())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = {X.columns[i]: pd.Categorical(X.iloc[:, i], categories=self.categories[i]).codes for i in range(X.shape[1])}\n",
    "        return pd.DataFrame(data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db50ed",
   "metadata": {},
   "source": [
    "We want this to be fast. I can get as much RAM as I will ever need through VMs on GCP, but that is not the point. I want to see how far I can push my local hardware, but this goes even beyond that.\n",
    "\n",
    "I need the speed to make best use of my time as I build a feel for what RecSys models are about. And the path to this leads through making the data I will work on smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94cd3410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 1.6 s, total: 21.2 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "transactions = pd.read_csv('data/transactions_train.csv', dtype={\"article_id\": \"str\"})\n",
    "customers = pd.read_csv('data/customers.csv')\n",
    "articles = pd.read_csv('data/articles.csv', dtype={\"article_id\": \"str\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa75de3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                      128\n",
       "t_dat               2129817708\n",
       "customer_id         3846387204\n",
       "article_id          2129817708\n",
       "price                254306592\n",
       "sales_channel_id     254306592\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0b35ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31788324 entries, 0 to 31788323\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   t_dat             object \n",
      " 1   customer_id       object \n",
      " 2   article_id        object \n",
      " 3   price             float64\n",
      " 4   sales_channel_id  int64  \n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "transactions.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d85685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.89 s, sys: 50.2 ms, total: 4.94 s\n",
      "Wall time: 4.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1362281"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "transactions['customer_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ef4c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 s, sys: 1.48 s, total: 20.4 s\n",
      "Wall time: 20.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1362281"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "transactions['customer_id'] = customer_hex_id_to_int(transactions['customer_id'])\n",
    "transactions['customer_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3733aeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                      128\n",
       "t_dat               2129817708\n",
       "customer_id          254306592\n",
       "article_id          2129817708\n",
       "price                254306592\n",
       "sales_channel_id     254306592\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe293c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31788324 entries, 0 to 31788323\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   t_dat             object \n",
      " 1   customer_id       uint64 \n",
      " 2   article_id        object \n",
      " 3   price             float64\n",
      " 4   sales_channel_id  int64  \n",
      "dtypes: float64(1), int64(1), object(2), uint64(1)\n",
      "memory usage: 4.7 GB\n"
     ]
    }
   ],
   "source": [
    "transactions.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de972b0b",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Initially, I wanted to get rid of the `t_dat` column but on second thought I am not a fan.\n",
    "\n",
    "I am all for speed and reducing weight, but the main purpose of this activity is to increase developer productivity.\n",
    "\n",
    "If I fall back down to ints representing year, week, day I will be certainly trading developer productivity for fewer CPU cycles that are needed (and I want to go in the exact opposite direction! developer productivity > (nearly) anything else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9a0cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 s, sys: 250 ms, total: 2.05 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transactions.t_dat = pd.to_datetime(transactions.t_dat, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bfae010",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['week'] = 104 - (transactions.t_dat.max() - transactions.t_dat).dt.days // 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90457b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31788324 entries, 0 to 31788323\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   t_dat             datetime64[ns]\n",
      " 1   customer_id       uint64        \n",
      " 2   article_id        object        \n",
      " 3   price             float64       \n",
      " 4   sales_channel_id  int64         \n",
      " 5   week              int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(1), uint64(1)\n",
      "memory usage: 3.2 GB\n"
     ]
    }
   ],
   "source": [
    "transactions.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094879d",
   "metadata": {},
   "source": [
    "Let's do something about the `article_id` (both here and on `articles`) and let's take a closer look at `price`, `sales_channel_id` and `week`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7390ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.article_id = article_id_str_to_int(transactions.article_id)\n",
    "articles.article_id = article_id_str_to_int(articles.article_id)\n",
    "\n",
    "transactions.week = transactions.week.astype('int8')\n",
    "transactions.sales_channel_id = transactions.sales_channel_id.astype('int8')\n",
    "transactions.price = transactions.price.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c28d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31788324 entries, 0 to 31788323\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   t_dat             datetime64[ns]\n",
      " 1   customer_id       uint64        \n",
      " 2   article_id        int32         \n",
      " 3   price             float32       \n",
      " 4   sales_channel_id  int8          \n",
      " 5   week              int8          \n",
      "dtypes: datetime64[ns](1), float32(1), int32(1), int8(2), uint64(1)\n",
      "memory usage: 788.2 MB\n"
     ]
    }
   ],
   "source": [
    "transactions.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10623914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31788324 entries, 0 to 31788323\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   customer_id       uint64 \n",
      " 1   article_id        int32  \n",
      " 2   price             float32\n",
      " 3   sales_channel_id  int8   \n",
      " 4   week              int8   \n",
      "dtypes: float32(1), int32(1), int8(2), uint64(1)\n",
      "memory usage: 545.7 MB\n"
     ]
    }
   ],
   "source": [
    "transactions.drop(columns='t_dat').info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f874dac4",
   "metadata": {},
   "source": [
    "Well, this is interesting. There are very few unique `t_dat` values hence despite it being a scary `datetime64` it takes up very little memory!\n",
    "\n",
    "Keeping it for convenience is definitely the way to go.\n",
    "\n",
    "Let's take a brief look at the `customers` and `articles` dfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb06d877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1371980 entries, 0 to 1371979\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   customer_id             1371980 non-null  object \n",
      " 1   FN                      476930 non-null   float64\n",
      " 2   Active                  464404 non-null   float64\n",
      " 3   club_member_status      1365918 non-null  object \n",
      " 4   fashion_news_frequency  1355971 non-null  object \n",
      " 5   age                     1356119 non-null  float64\n",
      " 6   postal_code             1371980 non-null  object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 512.3 MB\n"
     ]
    }
   ],
   "source": [
    "customers.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55f885f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105542 entries, 0 to 105541\n",
      "Data columns (total 25 columns):\n",
      " #   Column                        Non-Null Count   Dtype \n",
      "---  ------                        --------------   ----- \n",
      " 0   article_id                    105542 non-null  int32 \n",
      " 1   product_code                  105542 non-null  int64 \n",
      " 2   prod_name                     105542 non-null  object\n",
      " 3   product_type_no               105542 non-null  int64 \n",
      " 4   product_type_name             105542 non-null  object\n",
      " 5   product_group_name            105542 non-null  object\n",
      " 6   graphical_appearance_no       105542 non-null  int64 \n",
      " 7   graphical_appearance_name     105542 non-null  object\n",
      " 8   colour_group_code             105542 non-null  int64 \n",
      " 9   colour_group_name             105542 non-null  object\n",
      " 10  perceived_colour_value_id     105542 non-null  int64 \n",
      " 11  perceived_colour_value_name   105542 non-null  object\n",
      " 12  perceived_colour_master_id    105542 non-null  int64 \n",
      " 13  perceived_colour_master_name  105542 non-null  object\n",
      " 14  department_no                 105542 non-null  int64 \n",
      " 15  department_name               105542 non-null  object\n",
      " 16  index_code                    105542 non-null  object\n",
      " 17  index_name                    105542 non-null  object\n",
      " 18  index_group_no                105542 non-null  int64 \n",
      " 19  index_group_name              105542 non-null  object\n",
      " 20  section_no                    105542 non-null  int64 \n",
      " 21  section_name                  105542 non-null  object\n",
      " 22  garment_group_no              105542 non-null  int64 \n",
      " 23  garment_group_name            105542 non-null  object\n",
      " 24  detail_desc                   105126 non-null  object\n",
      "dtypes: int32(1), int64(10), object(14)\n",
      "memory usage: 117.2 MB\n"
     ]
    }
   ],
   "source": [
    "articles.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf9c76",
   "metadata": {},
   "source": [
    "Well, this stuff will be getting merged with our transactions df at some point, so I guess we can also make this smaller and easier to work with down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efd82f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ACTIVE', nan, 'PRE-CREATE', 'LEFT CLUB'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers['club_member_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61bf9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.customer_id = customer_hex_id_to_int(customers.customer_id)\n",
    "for col in ['FN', 'Active', 'age']:\n",
    "    customers[col].fillna(-1, inplace=True)\n",
    "    customers[col] = customers[col].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "758411dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.club_member_status = Categorize().fit_transform(customers[['club_member_status']]).club_member_status\n",
    "customers.postal_code = Categorize().fit_transform(customers[['postal_code']]).postal_code\n",
    "customers.fashion_news_frequency = Categorize().fit_transform(customers[['fashion_news_frequency']]).fashion_news_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7761a45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1371980 entries, 0 to 1371979\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count    Dtype \n",
      "---  ------                  --------------    ----- \n",
      " 0   customer_id             1371980 non-null  uint64\n",
      " 1   FN                      1371980 non-null  int8  \n",
      " 2   Active                  1371980 non-null  int8  \n",
      " 3   club_member_status      1371980 non-null  int8  \n",
      " 4   fashion_news_frequency  1371980 non-null  int8  \n",
      " 5   age                     1371980 non-null  int8  \n",
      " 6   postal_code             1371980 non-null  int32 \n",
      "dtypes: int32(1), int8(5), uint64(1)\n",
      "memory usage: 22.2 MB\n"
     ]
    }
   ],
   "source": [
    "customers.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cb4fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in articles.columns:\n",
    "    if articles[col].dtype == 'object':\n",
    "        articles[col] = Categorize().fit_transform(articles[[col]])[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a8e33bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105542 entries, 0 to 105541\n",
      "Data columns (total 25 columns):\n",
      " #   Column                        Non-Null Count   Dtype\n",
      "---  ------                        --------------   -----\n",
      " 0   article_id                    105542 non-null  int32\n",
      " 1   product_code                  105542 non-null  int64\n",
      " 2   prod_name                     105542 non-null  int32\n",
      " 3   product_type_no               105542 non-null  int64\n",
      " 4   product_type_name             105542 non-null  int16\n",
      " 5   product_group_name            105542 non-null  int8 \n",
      " 6   graphical_appearance_no       105542 non-null  int64\n",
      " 7   graphical_appearance_name     105542 non-null  int8 \n",
      " 8   colour_group_code             105542 non-null  int64\n",
      " 9   colour_group_name             105542 non-null  int8 \n",
      " 10  perceived_colour_value_id     105542 non-null  int64\n",
      " 11  perceived_colour_value_name   105542 non-null  int8 \n",
      " 12  perceived_colour_master_id    105542 non-null  int64\n",
      " 13  perceived_colour_master_name  105542 non-null  int8 \n",
      " 14  department_no                 105542 non-null  int64\n",
      " 15  department_name               105542 non-null  int16\n",
      " 16  index_code                    105542 non-null  int8 \n",
      " 17  index_name                    105542 non-null  int8 \n",
      " 18  index_group_no                105542 non-null  int64\n",
      " 19  index_group_name              105542 non-null  int8 \n",
      " 20  section_no                    105542 non-null  int64\n",
      " 21  section_name                  105542 non-null  int8 \n",
      " 22  garment_group_no              105542 non-null  int64\n",
      " 23  garment_group_name            105542 non-null  int8 \n",
      " 24  detail_desc                   105542 non-null  int32\n",
      "dtypes: int16(2), int32(3), int64(10), int8(10)\n",
      "memory usage: 10.7 MB\n"
     ]
    }
   ],
   "source": [
    "articles.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3596527",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in articles.columns:\n",
    "    if articles[col].dtype == 'int64':\n",
    "        articles[col] = articles[col].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc411fb5",
   "metadata": {},
   "source": [
    "And this concludes our raw data preparation step! Let's now write everything back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86f4e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.sort_values(['t_dat', 'customer_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "682b1125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.85 s, sys: 220 ms, total: 3.07 s\n",
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transactions.to_parquet('data/transactions_train.parquet')\n",
    "customers.to_parquet('data/customers.parquet')\n",
    "articles.to_parquet('data/articles.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930753b7",
   "metadata": {},
   "source": [
    "Let's also generate a sample we will be able to use to speed up development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fd97303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 873 ms, sys: 19.1 ms, total: 892 ms\n",
      "Wall time: 885 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# let's create a 5% sample of the entiriety of the data to speed up dev\n",
    "\n",
    "sample = 0.05\n",
    "customers_sample = customers.sample(frac=sample, replace=False)\n",
    "customers_sample_ids = set(customers_sample['customer_id'])\n",
    "transactions_sample = transactions[transactions[\"customer_id\"].isin(customers_sample_ids)]\n",
    "articles_sample_ids = set(transactions_sample[\"article_id\"])\n",
    "articles_sample = articles[articles[\"article_id\"].isin(articles_sample_ids)]\n",
    "\n",
    "customers_sample.to_parquet(f'data/customers_sample_{sample}.parquet', index=False)\n",
    "transactions_sample.to_parquet(f'data/transactions_train_sample_{sample}.parquet', index=False)\n",
    "articles_sample.to_parquet(f'data/articles_train_sample_{sample}.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f8dc2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cc90c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "val_week_purchases_by_cust = default99dict(list)\n",
    "\n",
    "val_week_purchases_by_cust.update(\n",
    "    transactions[transactions.week == transactions.week.max()] \\\n",
    "        .groupby('customer_id')['article_id'] \\\n",
    "        .apply(list) \\\n",
    "        .to_dict()\n",
    ")\n",
    "\n",
    "pd.to_pickle(dict(val_week_purchases_by_cust), 'data/val_week_purchases_by_cust.pkl')\n",
    "\n",
    "sample_sub = pd.read_csv('data/sample_submission.csv')\n",
    "valid_gt = customer_hex_id_to_int(sample_sub.customer_id) \\\n",
    "    .map(val_week_purchases_by_cust) \\\n",
    "    .apply(lambda xx: ' '.join('0' + str(x) for x in xx))\n",
    "\n",
    "sample_sub.prediction = valid_gt\n",
    "sample_sub.to_parquet('data/validation_ground_truth.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73bdc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from average_precision import apk\n",
    "\n",
    "def calculate_apk(list_of_preds, list_of_gts):\n",
    "    # for fast validation this can be changed to operate on dicts of {'cust_id_int': [art_id_int, ...]}\n",
    "    # using 'data/val_week_purchases_by_cust.pkl'\n",
    "    apks = []\n",
    "    for preds, gt in zip(list_of_preds, list_of_gts):\n",
    "        apks.append(apk(gt, preds, k=12))\n",
    "    return np.mean(apks)\n",
    "\n",
    "def eval_sub(sub_csv, skip_cust_with_no_purchases=True):\n",
    "    sub=pd.read_csv(sub_csv)\n",
    "    validation_set=pd.read_parquet('data/validation_ground_truth.parquet')\n",
    "\n",
    "    apks = []\n",
    "\n",
    "    no_purchases_pattern = []\n",
    "    for pred, gt in zip(sub.prediction.str.split(), validation_set.prediction.str.split()):\n",
    "        if skip_cust_with_no_purchases and (gt == no_purchases_pattern): continue\n",
    "        apks.append(apk(gt, pred, k=12))\n",
    "    return np.mean(apks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f1fca",
   "metadata": {},
   "source": [
    "## Strong starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df636501",
   "metadata": {},
   "source": [
    "This, without a doubt, has to be this kaggle [kernel](https://www.kaggle.com/hengzheng/time-is-our-best-friend-v2).\n",
    "\n",
    "These candidates are so good they get a decent score even with the very simple ranking applied!\n",
    "\n",
    "There is only one problem with this solution -- it doesn't structure the 'candidates' in a way where they could be fed into a ranking model. Plus the candidates have information they can't possibly have. If candidates are candidates for the future, they can't have access to the information on how well something will sell in the future week.\n",
    "\n",
    "We need to:\n",
    "- find a way to structure candidates so that they could be fed into a ranking model\n",
    "- create features that would capture the information the kernel is relying on\n",
    "- first output the predictions manually (hardcode a ranking model) and subsequently feed the data to a ranker\n",
    "\n",
    "In my previuos attempt I generated candidates for all users based on bestsellers for all weeks.\n",
    "\n",
    "But that created a lot of junk.\n",
    "\n",
    "This bestseller logic should be applied as postprocessing to a solution! The flow should be as follows:\n",
    "* predict on candidates\n",
    "* if for some customer the predictions are in some sense not reliable enough, or the likelihood of a sale is to low, use the bestseller logic from the kaggle kernel.\n",
    "\n",
    "Let us understand what the customers are doing a little bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70b97d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 ms, sys: 0 ns, total: 36.5 ms\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_week = transactions[transactions.week == transactions.week.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99018453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    68984.000000\n",
       "mean         3.483576\n",
       "std          3.535144\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          4.000000\n",
       "max        104.000000\n",
       "Name: customer_id, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_week['customer_id'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ac53f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestsellers_last_week = set(transactions[transactions.week == transactions.week.max()].article_id.value_counts().index[:12])\n",
    "bestsellers_week_ago = set(transactions[transactions.week == transactions.week.max()-1].article_id.value_counts().index[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "babc378e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.027967924897320556, 0.02349871624686344)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_week.article_id.isin(bestsellers_last_week).mean(), final_week.article_id.isin(bestsellers_week_ago).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fac6e",
   "metadata": {},
   "source": [
    "People **are** buying the bestsellers but not as much as one might think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e365a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "without_final_week = transactions[transactions.week != transactions.week.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39094204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 s, sys: 0 ns, total: 20.4 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_bought_items = 0\n",
    "last_purchase_repeated = 0\n",
    "purchases_cust_with_no_history = 0\n",
    "items_purchased_by_custs_with_no_history = []\n",
    "week_of_earlier_purchase = []\n",
    "i = 0\n",
    "\n",
    "for c_id, df in final_week.groupby('customer_id'):\n",
    "    purchases_final_week = set(df.article_id)\n",
    "    unique_bought_items += len(purchases_final_week)\n",
    "    \n",
    "    purchase_history = without_final_week[without_final_week.customer_id == c_id]\n",
    "    purchases_before = set(purchase_history[purchase_history.week == purchase_history.week.max()].article_id)\n",
    "    week_of_earlier_purchase.append(purchase_history.week.max())\n",
    "    \n",
    "    if len(purchases_before) == 0:\n",
    "        purchases_cust_with_no_history += len(purchases_final_week)\n",
    "        items_purchased_by_custs_with_no_history += list(purchases_final_week)\n",
    "    else:\n",
    "        last_purchase_repeated += len(purchases_final_week.intersection(purchases_before))\n",
    "    i += 1\n",
    "    if i == 1000: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e1ffb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wks_since_purchase = []\n",
    "for week in week_of_earlier_purchase:\n",
    "    if type(week) == np.int8:\n",
    "        wks_since_purchase.append(104 - week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a8412e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     0.196312\n",
       "2     0.305857\n",
       "3     0.396963\n",
       "4     0.475054\n",
       "5     0.539046\n",
       "7     0.591106\n",
       "6     0.634490\n",
       "8     0.676790\n",
       "13    0.708243\n",
       "10    0.738612\n",
       "11    0.767896\n",
       "12    0.790672\n",
       "9     0.812364\n",
       "16    0.827549\n",
       "15    0.838395\n",
       "23    0.849241\n",
       "14    0.860087\n",
       "18    0.870933\n",
       "21    0.880694\n",
       "17    0.889371\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weeks elapsed between the purchase in the final week and earlier purchase\n",
    "(pd.value_counts(wks_since_purchase)/len(wks_since_purchase)).head(20).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ab873d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3039, 80, 238)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_bought_items, last_purchase_repeated, purchases_cust_with_no_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74c97ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02100840336134454"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([itm in bestsellers_last_week for itm in items_purchased_by_custs_with_no_history])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e41f5c",
   "metadata": {},
   "source": [
    "There are not that many repeat purchases either. Though a vast majority of customers are repeat customers.\n",
    "\n",
    "And new customers are not buying bestsellers all that much either.\n",
    "\n",
    "I bet this could be improved if we did something useful with postal codes -- H&M operates across so many markets. The bestseller in one market doesn't have to be the bestseller in another.\n",
    "\n",
    "A good model should outpeform this simple last purchase heuristic by a large margin. Still, let's implement it to be able to use down the road to refine our solution for situations where we don't have enough data / results are inconclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e362ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 278 Âµs, total: 47.2 ms\n",
      "Wall time: 46.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "last_three_weeks = without_final_week[without_final_week.week > without_final_week.week.max()-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f09fe56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a slightly different logic to what's in the reference Kaggle kernel\n",
    "best_sellers = last_three_weeks.groupby('week').apply(lambda df: df.value_counts('article_id').index[:12].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05ae3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purchase_history_to_preds(df):\n",
    "    week_of_last_purchase = df.week.max()\n",
    "    last_purchased_basket = df[df.week == week_of_last_purchase]\n",
    "    purchased_items = last_purchased_basket.value_counts('article_id').index.tolist()\n",
    "    purchased_items += best_sellers[last_purchased_basket.week.head(1).item()]\n",
    "    return purchased_items[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "506e80d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 20s, sys: 670 ms, total: 2min 21s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cust2preds2 = last_three_weeks.groupby(['customer_id']).apply(purchase_history_to_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee4aa7",
   "metadata": {},
   "source": [
    "Mhmm that is a bit slow. Let's see if we can run this in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d79d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=24)\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97f8465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltw_dd = dd.from_pandas(last_three_weeks, npartitions=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5071cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.08 s, sys: 703 ms, total: 3.79 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cust2preds = ltw_dd.groupby('customer_id').apply(purchase_history_to_preds, meta=('x', 'object')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97eb5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b15f5",
   "metadata": {},
   "source": [
    "Let's generate a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5da82839",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_week = last_three_weeks.week.max()\n",
    "def get_preds_for_customer_id(c_id):\n",
    "    if c_id in c_ids_with_predictions:\n",
    "        pred_art_ids = cust2preds[c_id]\n",
    "    else:\n",
    "        pred_art_ids = best_sellers[last_week]\n",
    "    return  ['0' + str(art_id) for art_id in pred_art_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "158062c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.13 s, sys: 381 ms, total: 9.51 s\n",
      "Wall time: 9.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "c_ids_with_predictions = set(cust2preds.keys())\n",
    "preds = customer_hex_id_to_int(sample_sub.customer_id).map(get_preds_for_customer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7acde340",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.prediction = preds\n",
    "sample_sub.prediction = sample_sub.prediction.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "892950b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_name = 'bestsellers_single_week_logic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a0edcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 28.7 ms, total: 10.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_sub.to_csv(f'data/subs/{sub_name}.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08d3b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c h-and-m-personalized-fashion-recommendations -f 'data/subs/{sub_name}.csv.gz' -m {sub_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c9699",
   "metadata": {},
   "source": [
    "For completness sake I let me implement the logic exactly at it is on Kaggle, though I doubt this would make much of a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28b3237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe436b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltw_dd = dd.from_pandas(last_three_weeks, npartitions=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92657566",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = last_three_weeks.week.unique()\n",
    "best_sellers = {}\n",
    "for i in range(3):\n",
    "    best_sellers[weeks[i]] = last_three_weeks[last_three_weeks.week.isin(set(weeks[:i+1]))].article_id.value_counts('article_id').index.tolist()[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23663fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purchase_history_to_preds(df):\n",
    "    last_purchase_date = df.t_dat.max()\n",
    "    last_purchased_basket = df[df.t_dat == last_purchase_date]\n",
    "    purchased_items = last_purchased_basket.value_counts('article_id').index.tolist()\n",
    "    purchased_items += best_sellers[last_purchased_basket.week.head(1).item()]\n",
    "    return purchased_items[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d449967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.76 s, sys: 739 ms, total: 3.5 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cust2preds = ltw_dd.groupby('customer_id').apply(purchase_history_to_preds, meta=('x', 'object')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cac79625",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = customer_hex_id_to_int(sample_sub.customer_id).map(get_preds_for_customer_id)\n",
    "sample_sub.prediction = preds\n",
    "sample_sub.prediction = sample_sub.prediction.str.join(' ')\n",
    "\n",
    "sub_name = 'bestsellers_kernel_logic'\n",
    "\n",
    "sample_sub.to_csv(f'data/subs/{sub_name}.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a745d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020958093582425067"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sub(f'data/subs/{sub_name}.csv.gz', skip_cust_with_no_purchases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d379f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c h-and-m-personalized-fashion-recommendations -f 'data/subs/{sub_name}.csv.gz' -m {sub_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abdaf712",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
